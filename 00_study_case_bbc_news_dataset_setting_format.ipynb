{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset: BBC News. Setting Format. \n",
    "\n",
    "In this python notebook it will be analyzed a BBC News datasetto add a few columns that will be useful and to determine wich compression method fits best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import\n",
    "from DimCuantifier import DimCuantifier\n",
    "from PreProcessingDimCuantifier import PreProcessingDimCuantifier\n",
    "\n",
    "import gensim\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Model\n",
    "\n",
    "First, set word embeddings model, in this case normalized Glove with 42 billions of words and 300 dimensions.\n",
    "\n",
    "This will be useful for filtering out words from news content that do not appear in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create PreProceesingDimCuantifier object\n",
    "PreProcess = PreProcessingDimCuantifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize word embeddings or use already normalized word embeddings\n",
    "norm_glove_42B = 'normalized_glove.42B.300d.mod'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set current model of word embeddings\n",
    "current_model = gensim.models.KeyedVectors.load_word2vec_format(\n",
    "    norm_glove_42B, binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load dataset with pandas and see columns types and memory usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2225 entries, 0 to 2224\n",
      "Data columns (total 4 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   category  2225 non-null   object\n",
      " 1   filename  2225 non-null   object\n",
      " 2   title     2225 non-null   object\n",
      " 3   content   2225 non-null   object\n",
      "dtypes: object(4)\n",
      "memory usage: 69.7+ KB\n"
     ]
    }
   ],
   "source": [
    "filename = 'bbc-news-data.csv'\n",
    "bbc_news_dataset = pd.read_csv(filename, sep='\\t')\n",
    "bbc_news_dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>filename</th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>business</td>\n",
       "      <td>001.txt</td>\n",
       "      <td>Ad sales boost Time Warner profit</td>\n",
       "      <td>Quarterly profits at US media giant TimeWarne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>business</td>\n",
       "      <td>002.txt</td>\n",
       "      <td>Dollar gains on Greenspan speech</td>\n",
       "      <td>The dollar has hit its highest level against ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>business</td>\n",
       "      <td>003.txt</td>\n",
       "      <td>Yukos unit buyer faces loan claim</td>\n",
       "      <td>The owners of embattled Russian oil giant Yuk...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>business</td>\n",
       "      <td>004.txt</td>\n",
       "      <td>High fuel prices hit BA's profits</td>\n",
       "      <td>British Airways has blamed high fuel prices f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>business</td>\n",
       "      <td>005.txt</td>\n",
       "      <td>Pernod takeover talk lifts Domecq</td>\n",
       "      <td>Shares in UK drinks and food firm Allied Dome...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   category filename                              title  \\\n",
       "0  business  001.txt  Ad sales boost Time Warner profit   \n",
       "1  business  002.txt   Dollar gains on Greenspan speech   \n",
       "2  business  003.txt  Yukos unit buyer faces loan claim   \n",
       "3  business  004.txt  High fuel prices hit BA's profits   \n",
       "4  business  005.txt  Pernod takeover talk lifts Domecq   \n",
       "\n",
       "                                             content  \n",
       "0   Quarterly profits at US media giant TimeWarne...  \n",
       "1   The dollar has hit its highest level against ...  \n",
       "2   The owners of embattled Russian oil giant Yuk...  \n",
       "3   British Airways has blamed high fuel prices f...  \n",
       "4   Shares in UK drinks and food firm Allied Dome...  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bbc_news_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add new columns and save memory usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems like category can be cast to category type and filename column can be dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2225 entries, 0 to 2224\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count  Dtype   \n",
      "---  ------    --------------  -----   \n",
      " 0   category  2225 non-null   category\n",
      " 1   title     2225 non-null   object  \n",
      " 2   content   2225 non-null   object  \n",
      "dtypes: category(1), object(2)\n",
      "memory usage: 37.3+ KB\n"
     ]
    }
   ],
   "source": [
    "bbc_news_dataset['category'] = bbc_news_dataset['category'].astype('category')\n",
    "bbc_news_dataset = bbc_news_dataset.drop(['filename'], axis=1)\n",
    "bbc_news_dataset.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That decreased memory usage of the dataset.\n",
    "\n",
    "Now new columns will be added that can be useful for further analysis of the dataset:\n",
    "\n",
    "- *content_tokenized* : content of the news tokenized with stopwords removed and filtered to only keep words in word embeddings model\n",
    "- *original_len*: lenght of the original content of the news\n",
    "- *tokenized_len*: lenght of the new tokenized content of the news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use PreProcess.preprocess_document to tokenize and remove stopwords from the documents\n",
    "bbc_news_dataset['content_tokenized'] = bbc_news_dataset['content'].apply(lambda x: PreProcess.preprocess_document(x, current_model))\n",
    "bbc_news_dataset['original_len'] = bbc_news_dataset['content'].apply(lambda x: len(x.split()))\n",
    "bbc_news_dataset['tokenized_len'] = bbc_news_dataset['content_tokenized'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look again to memory usage and column types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2225 entries, 0 to 2224\n",
      "Data columns (total 6 columns):\n",
      " #   Column             Non-Null Count  Dtype   \n",
      "---  ------             --------------  -----   \n",
      " 0   category           2225 non-null   category\n",
      " 1   title              2225 non-null   object  \n",
      " 2   content            2225 non-null   object  \n",
      " 3   content_tokenized  2225 non-null   object  \n",
      " 4   original_len       2225 non-null   int64   \n",
      " 5   tokenized_len      2225 non-null   int64   \n",
      "dtypes: category(1), int64(2), object(3)\n",
      "memory usage: 89.4+ KB\n"
     ]
    }
   ],
   "source": [
    "bbc_news_dataset.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*original_len* and *tokenized_len* are *int64* and that might not be necessary, with pandas describe max and min value of every column will be displayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_len</th>\n",
       "      <th>tokenized_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2225.000000</td>\n",
       "      <td>2225.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>378.835955</td>\n",
       "      <td>229.438652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>238.220755</td>\n",
       "      <td>132.533164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>84.000000</td>\n",
       "      <td>47.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>240.000000</td>\n",
       "      <td>147.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>326.000000</td>\n",
       "      <td>202.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>466.000000</td>\n",
       "      <td>284.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4428.000000</td>\n",
       "      <td>2270.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       original_len  tokenized_len\n",
       "count   2225.000000    2225.000000\n",
       "mean     378.835955     229.438652\n",
       "std      238.220755     132.533164\n",
       "min       84.000000      47.000000\n",
       "25%      240.000000     147.000000\n",
       "50%      326.000000     202.000000\n",
       "75%      466.000000     284.000000\n",
       "max     4428.000000    2270.000000"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bbc_news_dataset.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both columns can be downcast to save memory usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbc_news_dataset[['original_len', 'tokenized_len']] = bbc_news_dataset[['original_len', 'tokenized_len']].apply(pd.to_numeric, downcast='integer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2225 entries, 0 to 2224\n",
      "Data columns (total 6 columns):\n",
      " #   Column             Non-Null Count  Dtype   \n",
      "---  ------             --------------  -----   \n",
      " 0   category           2225 non-null   category\n",
      " 1   title              2225 non-null   object  \n",
      " 2   content            2225 non-null   object  \n",
      " 3   content_tokenized  2225 non-null   object  \n",
      " 4   original_len       2225 non-null   int16   \n",
      " 5   tokenized_len      2225 non-null   int16   \n",
      "dtypes: category(1), int16(2), object(3)\n",
      "memory usage: 63.3+ KB\n"
     ]
    }
   ],
   "source": [
    "bbc_news_dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>content_tokenized</th>\n",
       "      <th>original_len</th>\n",
       "      <th>tokenized_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>business</td>\n",
       "      <td>Ad sales boost Time Warner profit</td>\n",
       "      <td>Quarterly profits at US media giant TimeWarne...</td>\n",
       "      <td>[quarterly, profits, us, media, giant, timewar...</td>\n",
       "      <td>415</td>\n",
       "      <td>250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>business</td>\n",
       "      <td>Dollar gains on Greenspan speech</td>\n",
       "      <td>The dollar has hit its highest level against ...</td>\n",
       "      <td>[dollar, hit, highest, level, euro, almost, th...</td>\n",
       "      <td>379</td>\n",
       "      <td>238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>business</td>\n",
       "      <td>Yukos unit buyer faces loan claim</td>\n",
       "      <td>The owners of embattled Russian oil giant Yuk...</td>\n",
       "      <td>[owners, embattled, russian, oil, giant, yukos...</td>\n",
       "      <td>258</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>business</td>\n",
       "      <td>High fuel prices hit BA's profits</td>\n",
       "      <td>British Airways has blamed high fuel prices f...</td>\n",
       "      <td>[british, airways, blamed, high, fuel, prices,...</td>\n",
       "      <td>400</td>\n",
       "      <td>265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>business</td>\n",
       "      <td>Pernod takeover talk lifts Domecq</td>\n",
       "      <td>Shares in UK drinks and food firm Allied Dome...</td>\n",
       "      <td>[shares, uk, drinks, food, firm, allied, domec...</td>\n",
       "      <td>260</td>\n",
       "      <td>163</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   category                              title  \\\n",
       "0  business  Ad sales boost Time Warner profit   \n",
       "1  business   Dollar gains on Greenspan speech   \n",
       "2  business  Yukos unit buyer faces loan claim   \n",
       "3  business  High fuel prices hit BA's profits   \n",
       "4  business  Pernod takeover talk lifts Domecq   \n",
       "\n",
       "                                             content  \\\n",
       "0   Quarterly profits at US media giant TimeWarne...   \n",
       "1   The dollar has hit its highest level against ...   \n",
       "2   The owners of embattled Russian oil giant Yuk...   \n",
       "3   British Airways has blamed high fuel prices f...   \n",
       "4   Shares in UK drinks and food firm Allied Dome...   \n",
       "\n",
       "                                   content_tokenized  original_len  \\\n",
       "0  [quarterly, profits, us, media, giant, timewar...           415   \n",
       "1  [dollar, hit, highest, level, euro, almost, th...           379   \n",
       "2  [owners, embattled, russian, oil, giant, yukos...           258   \n",
       "3  [british, airways, blamed, high, fuel, prices,...           400   \n",
       "4  [shares, uk, drinks, food, firm, allied, domec...           260   \n",
       "\n",
       "   tokenized_len  \n",
       "0            250  \n",
       "1            238  \n",
       "2            150  \n",
       "3            265  \n",
       "4            163  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bbc_news_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now four different methods of saving the file will be tried to compare memory usage and how fast they are. Those are csv, pickle, parquet, feather\n",
    "\n",
    "Download fastparquet and set it to pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fastparquet in c:\\users\\naxo7\\anaconda3\\envs\\python-cg\\lib\\site-packages (2023.2.0)\n",
      "Requirement already satisfied: cramjam>=2.3 in c:\\users\\naxo7\\anaconda3\\envs\\python-cg\\lib\\site-packages (from fastparquet) (2.6.2)\n",
      "Requirement already satisfied: numpy>=1.20.3 in c:\\users\\naxo7\\anaconda3\\envs\\python-cg\\lib\\site-packages (from fastparquet) (1.20.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\naxo7\\anaconda3\\envs\\python-cg\\lib\\site-packages (from fastparquet) (2023.1.0)\n",
      "Requirement already satisfied: pandas>=1.5.0 in c:\\users\\naxo7\\anaconda3\\envs\\python-cg\\lib\\site-packages (from fastparquet) (1.5.3)\n",
      "Requirement already satisfied: packaging in c:\\users\\naxo7\\anaconda3\\envs\\python-cg\\lib\\site-packages (from fastparquet) (20.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\naxo7\\anaconda3\\envs\\python-cg\\lib\\site-packages (from pandas>=1.5.0->fastparquet) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\naxo7\\anaconda3\\envs\\python-cg\\lib\\site-packages (from pandas>=1.5.0->fastparquet) (2020.1)\n",
      "Requirement already satisfied: six in c:\\users\\naxo7\\anaconda3\\envs\\python-cg\\lib\\site-packages (from packaging->fastparquet) (1.15.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\naxo7\\anaconda3\\envs\\python-cg\\lib\\site-packages (from packaging->fastparquet) (2.4.7)\n",
      "\n",
      "INSTALLED VERSIONS\n",
      "------------------\n",
      "commit           : 2e218d10984e9919f0296931d92ea851c6a6faf5\n",
      "python           : 3.8.5.final.0\n",
      "python-bits      : 64\n",
      "OS               : Windows\n",
      "OS-release       : 10\n",
      "Version          : 10.0.19041\n",
      "machine          : AMD64\n",
      "processor        : AMD64 Family 23 Model 17 Stepping 0, AuthenticAMD\n",
      "byteorder        : little\n",
      "LC_ALL           : None\n",
      "LANG             : None\n",
      "LOCALE           : Spanish_Belize.1252\n",
      "\n",
      "pandas           : 1.5.3\n",
      "numpy            : 1.20.3\n",
      "pytz             : 2020.1\n",
      "dateutil         : 2.8.1\n",
      "setuptools       : 49.6.0.post20200814\n",
      "pip              : 20.2.2\n",
      "Cython           : 0.29.23\n",
      "pytest           : None\n",
      "hypothesis       : None\n",
      "sphinx           : 3.2.1\n",
      "blosc            : None\n",
      "feather          : None\n",
      "xlsxwriter       : None\n",
      "lxml.etree       : None\n",
      "html5lib         : None\n",
      "pymysql          : None\n",
      "psycopg2         : None\n",
      "jinja2           : 2.11.2\n",
      "IPython          : 7.18.1\n",
      "pandas_datareader: None\n",
      "bs4              : None\n",
      "bottleneck       : 1.3.2\n",
      "brotli           : \n",
      "fastparquet      : 2023.2.0\n",
      "fsspec           : 2023.1.0\n",
      "gcsfs            : None\n",
      "matplotlib       : 3.4.2\n",
      "numba            : None\n",
      "numexpr          : 2.7.3\n",
      "odfpy            : None\n",
      "openpyxl         : None\n",
      "pandas_gbq       : None\n",
      "pyarrow          : 11.0.0\n",
      "pyreadstat       : None\n",
      "pyxlsb           : None\n",
      "s3fs             : None\n",
      "scipy            : 1.7.1\n",
      "snappy           : None\n",
      "sqlalchemy       : None\n",
      "tables           : None\n",
      "tabulate         : 0.8.9\n",
      "xarray           : None\n",
      "xlrd             : None\n",
      "xlwt             : None\n",
      "zstandard        : None\n",
      "tzdata           : None\n"
     ]
    }
   ],
   "source": [
    "!pip install fastparquet\n",
    "pd.io.parquet.get_engine('auto')\n",
    "pd.show_versions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save dataset in 4 different file types and time them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "531 ms ± 106 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "254 ms ± 26.4 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "236 ms ± 38.2 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "131 ms ± 16 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit bbc_news_dataset.to_csv('bbc_news_dataset_tokenized.csv', sep='\\t', index=False)\n",
    "%timeit bbc_news_dataset.to_pickle('bbc_news_dataset_tokenized.pkl')\n",
    "%timeit bbc_news_dataset.to_parquet('bbc_news_dataset_tokenized.parquet')\n",
    "%timeit bbc_news_dataset.to_feather('bbc_news_dataset_tokenized.feather')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "csv is clearly the slowest method, and feather the fastest.\n",
    "\n",
    "Now time reading those files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "139 ms ± 3.84 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "70.1 ms ± 2.4 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "126 ms ± 2.39 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "76.7 ms ± 2.07 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit bbc_news_dataset_csv = pd.read_csv('bbc_news_dataset_tokenized.csv', sep='\\t')\n",
    "%timeit bbc_news_dataset_pickle = pd.read_pickle('bbc_news_dataset_tokenized.pkl')\n",
    "%timeit bbc_news_dataset_parquet = pd.read_parquet('bbc_news_dataset_tokenized.parquet')\n",
    "%timeit bbc_news_dataset_feather = pd.read_feather('bbc_news_dataset_tokenized.feather')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, csv is the slowest. Pickle and feather are very fast\n",
    "\n",
    "Now compare memory usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2225 entries, 0 to 2224\n",
      "Data columns (total 6 columns):\n",
      " #   Column             Non-Null Count  Dtype \n",
      "---  ------             --------------  ----- \n",
      " 0   category           2225 non-null   object\n",
      " 1   title              2225 non-null   object\n",
      " 2   content            2225 non-null   object\n",
      " 3   content_tokenized  2225 non-null   object\n",
      " 4   original_len       2225 non-null   int64 \n",
      " 5   tokenized_len      2225 non-null   int64 \n",
      "dtypes: int64(2), object(4)\n",
      "memory usage: 104.4+ KB\n"
     ]
    }
   ],
   "source": [
    "bbc_news_dataset_csv.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2225 entries, 0 to 2224\n",
      "Data columns (total 6 columns):\n",
      " #   Column             Non-Null Count  Dtype   \n",
      "---  ------             --------------  -----   \n",
      " 0   category           2225 non-null   category\n",
      " 1   title              2225 non-null   object  \n",
      " 2   content            2225 non-null   object  \n",
      " 3   content_tokenized  2225 non-null   object  \n",
      " 4   original_len       2225 non-null   int16   \n",
      " 5   tokenized_len      2225 non-null   int16   \n",
      "dtypes: category(1), int16(2), object(3)\n",
      "memory usage: 63.2+ KB\n"
     ]
    }
   ],
   "source": [
    "bbc_news_dataset_pickle.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2225 entries, 0 to 2224\n",
      "Data columns (total 6 columns):\n",
      " #   Column             Non-Null Count  Dtype \n",
      "---  ------             --------------  ----- \n",
      " 0   category           2225 non-null   object\n",
      " 1   title              2225 non-null   object\n",
      " 2   content            2225 non-null   object\n",
      " 3   content_tokenized  2225 non-null   object\n",
      " 4   original_len       2225 non-null   int16 \n",
      " 5   tokenized_len      2225 non-null   int16 \n",
      "dtypes: int16(2), object(4)\n",
      "memory usage: 78.3+ KB\n"
     ]
    }
   ],
   "source": [
    "bbc_news_dataset_parquet.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2225 entries, 0 to 2224\n",
      "Data columns (total 6 columns):\n",
      " #   Column             Non-Null Count  Dtype   \n",
      "---  ------             --------------  -----   \n",
      " 0   category           2225 non-null   category\n",
      " 1   title              2225 non-null   object  \n",
      " 2   content            2225 non-null   object  \n",
      " 3   content_tokenized  2225 non-null   object  \n",
      " 4   original_len       2225 non-null   int16   \n",
      " 5   tokenized_len      2225 non-null   int16   \n",
      "dtypes: category(1), int16(2), object(3)\n",
      "memory usage: 63.3+ KB\n"
     ]
    }
   ],
   "source": [
    "bbc_news_dataset_feather.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is observed that csv method is the heaviest.\n",
    "\n",
    "Csv and parquet do not keep the types of the columns as were saved.\n",
    "\n",
    "Pickle and feather are the lightest and both keep types of columns\n",
    "\n",
    "Finally take a look and how this methods keep the list structure for the column *content_tokenized*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "quarterly\n"
     ]
    }
   ],
   "source": [
    "print(bbc_news_dataset_csv['content_tokenized'][0][0])\n",
    "print(eval(bbc_news_dataset_csv['content_tokenized'][0])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quarterly\n"
     ]
    }
   ],
   "source": [
    "print(bbc_news_dataset_pickle['content_tokenized'][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91\n",
      "b'[\"quarterly\",\"profits\",\"us\",\"media\",\"giant\",\"timewarner\",\"jumped\",\"76\",\"three\",\"months\",\"december\",\"year-earlier\",\"firm\",\"one\",\"biggest\",\"investors\",\"google\",\"benefited\",\"sales\",\"high-speed\",\"internet\",\"connections\",\"higher\",\"advert\",\"sales\",\"timewarner\",\"said\",\"fourth\",\"quarter\",\"sales\",\"rose\",\"2\",\"profits\",\"buoyed\",\"one-off\",\"gains\",\"offset\",\"profit\",\"dip\",\"warner\",\"bros\",\"less\",\"users\",\"aol\",\"time\",\"warner\",\"said\",\"friday\",\"owns\",\"8\",\"search-engine\",\"google\",\"internet\",\"business\",\"aol\",\"mixed\",\"fortunes\",\"lost\",\"464,000\",\"subscribers\",\"fourth\",\"quarter\",\"profits\",\"lower\",\"preceding\",\"three\",\"quarters\",\"however\",\"company\",\"said\",\"aol\",\"\\'s\",\"underlying\",\"profit\",\"exceptional\",\"items\",\"rose\",\"8\",\"back\",\"stronger\",\"internet\",\"advertising\",\"revenues\",\"hopes\",\"increase\",\"subscribers\",\"offering\",\"online\",\"service\",\"free\",\"timewarner\",\"internet\",\"customers\",\"try\",\"sign\",\"aol\",\"\\'s\",\"existing\",\"customers\",\"high-speed\",\"broadband\",\"timewarner\",\"also\",\"restate\",\"2000\",\"2003\",\"results\",\"following\",\"probe\",\"us\",\"securities\",\"exchange\",\"commission\",\"sec\",\"close\",\"concluding\",\"time\",\"warner\",\"\\'s\",\"fourth\",\"quarter\",\"profits\",\"slightly\",\"better\",\"analysts\",\"expectations\",\"film\",\"division\",\"saw\",\"profits\",\"slump\",\"27\",\"284m\",\"helped\",\"box-office\",\"flops\",\"alexander\",\"catwoman\",\"sharp\",\"contrast\",\"year-earlier\",\"third\",\"final\",\"film\",\"lord\",\"rings\",\"trilogy\",\"boosted\",\"results\",\"full-year\",\"timewarner\",\"posted\",\"profit\",\"27\",\"2003\",\"performance\",\"revenues\",\"grew\",\"6.4\",\"``\",\"financial\",\"performance\",\"strong\",\"meeting\",\"exceeding\",\"full-year\",\"objectives\",\"greatly\",\"enhancing\",\"flexibility\",\"\\'\\'\",\"chairman\",\"chief\",\"executive\",\"richard\",\"parsons\",\"said\",\"2005\",\"timewarner\",\"projecting\",\"operating\",\"earnings\",\"growth\",\"around\",\"5\",\"also\",\"expects\",\"higher\",\"revenue\",\"wider\",\"profit\",\"margins\",\"timewarner\",\"restate\",\"accounts\",\"part\",\"efforts\",\"resolve\",\"inquiry\",\"aol\",\"us\",\"market\",\"regulators\",\"already\",\"offered\",\"pay\",\"300m\",\"settle\",\"charges\",\"deal\",\"review\",\"sec\",\"company\",\"said\",\"unable\",\"estimate\",\"amount\",\"needed\",\"set\",\"aside\",\"legal\",\"reserves\",\"previously\",\"set\",\"500m\",\"intends\",\"adjust\",\"way\",\"accounts\",\"deal\",\"german\",\"music\",\"publisher\",\"bertelsmann\",\"\\'s\",\"purchase\",\"stake\",\"aol\",\"europe\",\"reported\",\"advertising\",\"revenue\",\"book\",\"sale\",\"stake\",\"aol\",\"europe\",\"loss\",\"value\",\"stake\"]'\n",
      "quarterly\n"
     ]
    }
   ],
   "source": [
    "print(bbc_news_dataset_parquet['content_tokenized'][0][0])\n",
    "print(bbc_news_dataset_parquet['content_tokenized'][0])\n",
    "print(eval(bbc_news_dataset_parquet['content_tokenized'][0])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quarterly\n"
     ]
    }
   ],
   "source": [
    "print(bbc_news_dataset_feather['content_tokenized'][0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Only pickle and feather keep list structure.\n",
    "\n",
    "In summary, pickle and feather are the lightest and keep structure of the columns, even list structure.\n",
    "Pickle is a bit faster for reading and feather is much faster for saving.\n",
    "But, since this dataset will be read more than saved, pickle is chosen"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "eb43bf7d185075f68ab58edfc590fc92d6787729092cf19766c04a8e1e005643"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
